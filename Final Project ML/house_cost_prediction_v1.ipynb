{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data = data.drop('Id', axis = 1)\n",
    "#display(data.head(n=20))\n",
    "\n",
    "raw_features = data.drop(\"SalePrice\", axis = 1)\n",
    "labels = data[\"SalePrice\"]\n",
    "#display(raw_features.head(n=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(ypred, ytest) : \n",
    "    assert len(ytest) == len(ypred)\n",
    "    return np.sqrt(np.mean((np.log1p(ypred) - np.log1p(ytest))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nan_values(data):\n",
    "    limit = int(len(data) * 0.75)\n",
    "    \n",
    "    for columns in data.columns.tolist():\n",
    "        if pd.isnull(data[columns]).any(axis = 0) == True:\n",
    "            no_of_na = 0\n",
    "            for value in data[columns]:\n",
    "                if pd.isna(value) == True:\n",
    "                    no_of_na += 1\n",
    "\n",
    "            #print(columns)\n",
    "            #print(\"{} missing values from {}.\".format(no_of_na, int(len(data))))\n",
    "\n",
    "            if no_of_na > limit:\n",
    "                #print(\"Drop\")\n",
    "                data.drop(columns, axis = 1, inplace = True)\n",
    "            else:\n",
    "                #print(\"Replace\")\n",
    "                value = data[columns].value_counts().index.tolist()\n",
    "                data = data.fillna({columns: value[0]})\n",
    "    \n",
    "    \n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = solve_nan_values(data = raw_features)\n",
    "#raw_features.head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = raw_features.select_dtypes(include = ['object']).copy()\n",
    "#categorical_data.head(n = 20)\n",
    "\n",
    "for columns in categorical_data.columns.tolist():\n",
    "    categories = categorical_data[columns]\n",
    "    categorical_data.drop(columns, axis = 1, inplace = True)\n",
    "    dummy = pd.get_dummies(categories)\n",
    "    categorical_data = pd.concat([categorical_data, dummy], axis = 1)\n",
    "    \n",
    "\n",
    "#categorical_data.head(n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features.drop(raw_features.select_dtypes(['object']), inplace = True, axis = 1)\n",
    "\n",
    "raw_features = pd.concat([raw_features, categorical_data], axis = 1)\n",
    "\n",
    "corr = raw_features.corr()\n",
    "#sn.heatmap(corr)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = raw_features.corr().abs()\n",
    "\n",
    "#print(corr_matrix)\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_high = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "# Drop features \n",
    "raw_features.drop(to_high, axis = 1, inplace = True)\n",
    "\n",
    "#raw_features.head(n = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = preprocessing.scale(raw_features)\n",
    "\n",
    "\n",
    "x = raw_features.iloc[:,:-1].values\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "features = standard_scaler.fit_transform(x)\n",
    "\n",
    "from sklearn import decomposition\n",
    "import seaborn as sn\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pca.n_components = 2\n",
    "pca_data = pca.fit_transform(features)\n",
    "\n",
    "\n",
    "#print(\"Shape of PCA = \", pca_data.shape)\n",
    "\n",
    "pca_data = np.vstack((pca_data.T, labels)).T\n",
    "\n",
    "#print(\"Shape of PCA = \", pca_data.shape)\n",
    "\n",
    "pca_df = pd.DataFrame(data = pca_data, columns = (\"First Principal\", \"Second Principal\", \"Label\"))\n",
    "\n",
    "for idx in range(len(pca_df)):\n",
    "    for counter in range(100):\n",
    "        lower_price = counter * 10000\n",
    "        upper_price = (counter + 1) * 10000\n",
    "        if pca_df.at[idx, \"Label\"] > lower_price and pca_df.at[idx, \"Label\"] < upper_price:\n",
    "            pca_df.at[idx, \"Label\"] = upper_price\n",
    "            break\n",
    "\n",
    "sn.FacetGrid(pca_df, hue = \"Label\", size = 10).map(plt.scatter, \"First Principal\", \"Second Principal\").add_legend()\n",
    "plt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size = 0.75, test_size = 0.25, random_state = 42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "\n",
    "\n",
    "def random_forest_param_selection(X, y):\n",
    "    \n",
    "    rfc = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    param_grid = \n",
    "    {\n",
    "        'n_estimators': [750, 1000],\n",
    "        'max_depth' :  [50, 100],\n",
    "    }\n",
    "    \n",
    "    folds = KFold(n_splits = 5, random_state = 42)\n",
    "    scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "    clf = GridSearchCV(rfc, param_grid, cv = folds, scoring = scorer)\n",
    "    cross_score = cross_val_score(estimator = clf, X = X, y = y, cv = folds)\n",
    "    meanScore = np.average(cross_score)\n",
    "    print(\"Training Score: \", meanScore)\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    print('Best score: {}'.format(clf.best_score_))\n",
    "    print('Best parameters: {}'.format(clf.best_params_))\n",
    "    \n",
    "    return clf.best_params_\n",
    "\n",
    "\n",
    "#dictionary = random_forest_param_selection(X_train, y_train)\n",
    "#n_estimators = dictionary['n_estimators']\n",
    "#max_depth = dictionary['max_depth']\n",
    "#n_estimators = n_estimators, max_depth = max_depth\n",
    "clf1 = RandomForestClassifier(n_estimators = 1000, max_depth = 50)\n",
    "ada = AdaBoostClassifier(n_estimators = 20, base_estimator = clf1, learning_rate=0.5)\n",
    "ada.fit(X_train,y_train)\n",
    "y_predict = ada.predict(X_test)\n",
    "error = rmsle(y_test, y_predict)\n",
    "print (\"RandomForestClassifier Error:\",error)\n",
    "#RandomForestClassifier Error: 0.3507037397855725\n",
    "\n",
    "'''Training set has 1095 samples.\n",
    "Testing set has 365 samples.\n",
    "Training Score:  -0.19904423237553598\n",
    "Best score: -0.1970983381825009\n",
    "Best parameters: {'max_depth': 50, 'n_estimators': 1000}\n",
    "RandomForestClassifier Error: 0.1881650096768023'''\n",
    "\n",
    "#RandomForestClassifier Error: 0.1733914972370594\n",
    "\n",
    "#0.1858308350826308\n",
    "#0.1830331837122414\n",
    "#0.23818856495254281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_param_selection(X, y):\n",
    "    print(\"START\")\n",
    "    k_range = list(range(1, 100))\n",
    "    weight_options = [\"uniform\", \"distance\"]\n",
    "    \n",
    "    param_grid = dict(n_neighbors = k_range, weights = weight_options)\n",
    "    algorithm = KNeighborsClassifier()\n",
    "    \n",
    "    folds = KFold(n_splits = 5, random_state=42)\n",
    "    scorer = make_scorer(rmsle, greater_is_better = False)\n",
    "    clf = GridSearchCV(algorithm, param_grid, cv = folds, scoring = scorer)\n",
    "    cross_score = cross_val_score(estimator = algorithm, X = X, y = y, cv = folds)\n",
    "    meanScore = np.average(cross_score)\n",
    "    print(\"Training Score: \", meanScore)\n",
    "    \n",
    "    clf.fit(X, y.ravel())\n",
    "    print('Best score: {}'.format(clf.best_score_))\n",
    "    print('Best parameters: {}'.format(clf.best_params_))\n",
    "    print(\"STOP\")\n",
    "    return clf.best_params_\n",
    "\n",
    "dictionary = knn_param_selection(X_train, y_train)\n",
    "k_neighbors = dictionary['n_neighbors']\n",
    "k_weights = dictionary['weights']\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors = k_neighbors, weights = k_weights)\n",
    "clf1.fit(X_train,y_train)\n",
    "y_predict = clf1.predict(X_test)\n",
    "error = rmsle(y_test, y_predict)\n",
    "print (\"KNeighborsClassifier Error:\", error)\n",
    "\n",
    "#START\n",
    "#Training Score:  0.0136986301369863\n",
    "#Best score: -0.2502592884857419\n",
    "#Best parameters: {'n_neighbors': 22, 'weights': 'distance'}\n",
    "#STOP\n",
    "#KNeighborsClassifier Error: 0.2628318814728741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_param_selection(X, y):\n",
    "    print(\"START\")\n",
    "    Cs = [0.01, 0.001, 0.1, 1]\n",
    "    gammas = [0.01, 0.1, 0.001, 1]\n",
    "    kernels = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "    \n",
    "    param_grid = {'C': Cs, 'gamma' : gammas, 'kernel': kernels}\n",
    "    algorithm = svm.SVC()\n",
    "    \n",
    "    \n",
    "    folds = KFold(n_splits = 5, random_state=42)\n",
    "    scorer = make_scorer(rmsle, greater_is_better = False)\n",
    "    clf = GridSearchCV(algorithm, param_grid, cv = folds, scoring = scorer)\n",
    "    cross_score = cross_val_score(estimator = algorithm, X = X, y = y, cv = folds)\n",
    "    meanScore = np.average(cross_score)\n",
    "    print(\"Training Score: \", meanScore)\n",
    "    \n",
    "    clf.fit(X, y.ravel())\n",
    "    print('Best score: {}'.format(clf.best_score_))\n",
    "    print('Best parameters: {}'.format(clf.best_params_))\n",
    "    print(\"STOP\")\n",
    "    return clf.best_params_\n",
    "    \n",
    "\n",
    "dictionary = svc_param_selection(X_train, y_train)\n",
    "print(dictionary['C']) \n",
    "print(dictionary['gamma']) \n",
    "print(dictionary['kernel']) \n",
    "clf = svm.SVC(C = dictionary['C'], gamma = dictionary['gamma'], kernel = dictionary['kernel'])\n",
    "clf.fit(X_train,y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "error = rmsle(y_test, y_predict)\n",
    "print (\"Support Vector Machine Error:\", error)\n",
    "\n",
    "#START\n",
    "#Training Score:  0.01278538812785388\n",
    "#Best score: -0.24738287881866586\n",
    "#Best parameters: {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}\n",
    "#STOP\n",
    "\n",
    "#Support Vector Machine Error: 0.2156940509120464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_param_selection(X, y):\n",
    "    print(\"START\")\n",
    "    algorithm = MLPClassifier(random_state = 42)\n",
    "    param_grid = {\n",
    "          'activation' : ['relu', 'identity', 'logistic', 'tanh'],    \n",
    "          'solver': ['lbfgs', 'sgd', 'adam'], \n",
    "          'learning_rate' : ['adaptive', 'constant'],\n",
    "          'alpha': 10.0 ** - np.arange(2, 4), \n",
    "          'hidden_layer_sizes': [700, 1000, 1200], \n",
    "          'max_iter': [1000], \n",
    "     }\n",
    "\n",
    "    \n",
    "    folds = KFold(n_splits=3, random_state=42)\n",
    "    scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "    clf = GridSearchCV(algorithm, param_grid, cv = folds, scoring = scorer)\n",
    "    cross_score = cross_val_score(estimator = clf, X = X, y = y, cv = folds)\n",
    "    meanScore = np.average(cross_score)\n",
    "    print(\"Training Score: \", meanScore)\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    print('Best score: {}'.format(clf.best_score_))\n",
    "    print('Best parameters: {}'.format(clf.best_params_))\n",
    "    print(\"STOP\")\n",
    "    return clf.best_params_\n",
    "\n",
    "dictionary = mlp_param_selection(X_train, y_train)\n",
    "activation = dictionary['activation']\n",
    "solver = dictionary['solver']\n",
    "learning_rate = dictionary['learning_rate']\n",
    "alpha = dictionary['alpha']\n",
    "hidden_layer_sizes = dictionary['hidden_layer_sizes']\n",
    "max_iter = dictionary['max_iter']\n",
    "\n",
    "clf1 = MLPClassifier(activation = activation, solver = solver, learning_rate = learning_rate, max_iter = max_iter, alpha = alpha, hidden_layer_sizes = hidden_layer_sizes, random_state = 42)\n",
    "clf1.fit(X_train,y_train)\n",
    "y_predict = clf1.predict(X_test)\n",
    "error = rmsle(y_test, y_predict)\n",
    "print (\"MLPClassifier Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
